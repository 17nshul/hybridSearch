from bm25 import BM25Plus, Tokenizer
from embeddings import Embeddings, ReRank
import numpy as np
import requests

resumes = [
    "aadit chugh ~5 yrs of experience as a data engineer professional with a demonstrated aptitude in data platform & analytics across  domains including edtech & automotive. aaditchugh00@gmail.com ❖ 8588825798 ❖ https://www.linkedin.com/in/aadit-chugh/ work experience embibe ( jio ) feb. 2022 – pr esent data engineer - ii bengaluru ▪ designed & developed a spring boot kafka consumer for real-time processing, transfor mation and ingestion of diverse embibe platfor m contents into elasticsearc h / mong odb / kafka ▪ developed restful apis using both java with spring boot and python with fastapi to address upstream data analytics requirements ▪ engineered data pipelines leveraging spark for real-time & batch processing and ingestion ▪ designed and implemented comprehensi ve data models , encompassing facts and dimensions , to suppor t both b2b / b2c analytics and reporting requirements ▪ enhanced workflow efficiency by migrating cron jobs to apac he airflow, resulting in automated data processing tasks and streamlined workflow manag ement ▪ enhanced the speed and efficiency of various apis by 60%, overseeing the delivery of data to upstream systems via refactoring and optimization. persistent systems oct 2020 – feb 2022 lead software engineer - data practice (client - embibe ) bengaluru ▪ manag ed end-to-end data ingestion in azure data explorer (adx) from diverse sources , overseeing data mapping, database policies , materialized view creation, resource manag ement, functions development and seamless integ ration of kusto kafka connect for streamlined data flow ▪ developed kql and sql queries to effecti vely address diverse analytical use cases and drive data-dri ven decision making ▪ contributed to the development of scala spark framew ork with a config-based system, capable of reading from multiple sources and writing to various sinks ▪ built a high-perfor mance spring boot rest service for collecting and storing events in azure event hub, implementing logging, monitoring, tracing functionalities and comprehensi ve unit tests ▪ impro ved data storag e and retriev al processes , leading to 50% reduction in data access latency ▪ perfor med migration of data and pipelines within azure across multiple regions leveraging various azure services ▪ developed comprehensi ve tableau dashboards to provide an overview of product perfor mance , showcasing user activities , key metrics and suppor ting strategic decision-making hyundai mobis r&d centr e march 2018 – may 2020 research engineer hyderabad ▪ built an end-to-end web-based project in python (flask) for simulation of cnn logic for detecting pedestrians ▪ extracted relevant signals from sensor (camera, radar , lidar) data leveraging spark for visualization ▪ developed an apac he nifi data pipeline with real-time rest api control for data flow between systems ▪ handling real-time sensor data ingestion and querying in apac he druid skills ▪ python, sql, java, kql ▪ spark, kafka, mong odb, elasticsearc h, redis ▪ spring boot, flask/f astapi, bitbuc ket, jenkins , docker, ci/cd , airflow ▪ azure , azure data explorer (adx), event hubs , synapse , adls , tableau education c-d ac acts feb, 2018 pg diploma - big data analytics pune ▪ grade a, 81% ▪ python, statistical analysis with r, ml, big data technologies , java with scala, dbms , mysql, mong odb, bi, data visualization (tableau), linux prog ramming, cloud computing ▪ project - effect of macroeconomic variables on indian stock mark et through time-series analysis and data reduction technique of factor analysis , tech used: python, r, tableau msit , gur u gobind singh indraprastha uni versity july, 2017 b.tech - computer science & engineering - 74% delhi",
    "profile summary overall 3.8+ years of experience as a qa engineer at times internet limited ( times of india group ), specializing in both manual and automation testing. my skill set extends to crafting automation scripts in selenium with java, particularly for highly transactional education, management, and media domain applications furthermore over time, i have actively maintained the selenium and java automation code and resources on github, consistently seeking improvements & integrating new features. i am consistently staying updated with emerging technologies & exploring new automation tools. work experience times internet limited ( times of india ) | noida, india qa engineer | 03/2023 - present project-3 : times of india 20+ channels backend development team --> slike (bennett coleman & co. ltd ) conducting comprehensive testing of slike cms studio, times of india in-house audio-video platform. automated manual testing using java and selenium to reduce the time taken for regression testing by 70%. collaborating with teams to execute test plans, ensuring seamless integration of all video solutions and workflows.providing backend support for times of india's 20+ toi channels, ensuring the platform's stability. project-2: times group web studio --> simulive conducted black box testing on network connectivity profiles of embedded systems with a 95% success rate working on, windows mobile, android, and ios platforms. implemented automated testing processes, reducing testing time by 70% & increasing team productivity 40%. involved in automation environment setup using eclipse, java, selenium webdriver, testng, qtp/uft, & github. skills: familiarity with agile methodology, knowledge of automation testing tools, selenium with java, selenium wait, selenium assertion, automation framework, testng, pom, api postman, github, devops tool ci/cd jenkins, jira, autoit, locators, hand on experience oops concepts, apache maven, xpath, rest assured, swagger, etc. inquipo technologies | bengaluru, karnataka software test engineer | 12/2019 - 03/2023 project-1: gmax - providing a testing perspective that helped our client's business manage hr department details effortlessly and in an organized manner and helped to accomplish the business goal of a company. -web application automates soapui functions within project.-involvement in all stages of the testing process manual testing to perform gui, functional testing, integration, regression testing, sanity testing, acceptance testing, performance & e2e testing. skills: analytical skills, manual and automation testing, system, function testing, blackbox testing, sdlc, stlc, dlc. technical skills agile methodology, sdlc, hybrid framework, java, oop, selenium webdriver, testng, maven, github, jenkins, jira, api postman, assertions, defect/bug life cycle, core java, browserstack, rest assured, basic auth, filezilla, json, rca, xml, html, collections, aws, eclipse, docker, windows os, mysql, mac os, junit, uat, time management, teamwork education history bhaskaracharya college of applied sciences | (bachelor of science &technology) delhi, india electronics and instrumentation engineering | 05/2017 final year project for line follower robot selected for international tech competition.automation certifaction securing 1st position in project during technical fest in delhi university fest project exhibition. microsoft sql server indo-germanic national conference on e-waste sustainability.indo-german collaboration aws: going cloud-native courses & certifications 1.)completed 1year offline training in software testing, 2.) dca, advance diploma in computer applications., 3.) aws fundamentals: going cloud-native, 4.) selenium webdriver with java basics&advanced +frameworks, 5.) microsoft sql server - an introducation online course, 6.) attended a seminar on tryst tech conducted by iit-delhiaakash nath ( software test engineer | qa engineer ) 8285717238 | aakash.nath4it@gmail.com | delhi, india | linked id : https://www.linkedin.com/in/aakash-nath-3048111b6 github id: https://github.com/akash8285717238",
    "sr devops engineer (sde -ii) year of birth: 1996 current location: bangalore, india availability: yes mobile: +91-7019782495 email: itsabhay00093@gmail.com https://www.linkedin.com/in/abhay-sharma-3b097469/ motivation cka certified sr. devops engineer with 7+ years of extensive experience in devops, infrastructure, containerization,cloud computing, middleware,operating system (linux system) and team management. capable of handling the client’s needs by translating them into appropriate technical solutions and providing motivation, guidance to both colleagues and clients. reliable self- starter with the ability to plan, design, implement, and manage small, medium, and large systems. good communication skills, interpersonal skills, self-motivated, quick learner. possess strong troubleshooting, problem solving and analytical skills.  summary: 7+ years of it industry experience encompassing a wide range of skill sets, roles, and industry verticals.  proficient in analyzing and translating business requirements to technical requirements and architecture.  run the production environment by monitoring availability and taking a holistic view of system health. gather and analyze metrics from operating systems as well as applications to assist in performance tuning and fault finding. strong design and implementation skills on public and private cloud, container based immutable deployments, sdlc acceleration through devops ci/cd process. experience in integrating ci-cd tools.  experience in monitoring tool like new relic, grafana, prometheus and app dynamics. experience in logging tool elk stack. experience in set up and managing various clients over cloud environments like aws, ibm cloud, and private cloud. experience in managing and setting up cloud kubernetes service and kubernetes cluster in private cloud environments.  having hands-on experience in container orchestration technology – kubernetes and deploying microservice applications on kubernetes. good communication skills, over interpersonal skills, self-motivated, quick learner, team player. knowledge on certificates and certificate chains.  expertise cloud: amazon web services (aws), ibm cloud public, azure, hybrid cloud. containerization: docker orchestration tool: kubernetes, helm scm tool: gitlab, github, bitbucket build server: jenkins, harness, bamboo apm: appdynamics, gcp monitoring o/s: rhel, ubuntu and windows application server: ibm websphere application server liberty, apache tomcat mobility platform: mobile first platform 7.x and 8.x devops tools: jira, gitlab, github, git, jenkins, urban code, elk, ansible, terraform, github actions, prometheus, grafana etc.  scripting: shell scripting, powershell professional experience assignments feb 2022 – till today, sr. devops engineer in tata cliq. oct 2020 – feb 2022, devops engineer in slk software dec 2019 – oct 2020, devops engineer in ntt ltd april 2016 – dec 2019, devops engineer ibm india pvt. ltd  project name: tata cliq job role: owning the complete dev and qa environment end to end and insuring there is no issue in all the 14 environments. configure the zero-downtime deployment (ci-cd) for the commerce and other microservices. experience in kubernetes to deploy scale, load balance and manage docker containers with multiple name spaced versions. trace complex ci/cd scenarios, release issues and environment issues in a multi-component environment. develop, maintain, and enhance key parts of the release procedures and processes. coordinate release activities with release management, project management, qa, and development teams to ensure a smooth roll out of releases. configure end to end traffic for an application from akamai/cloudflare to internal load balancer in aws. create sustainable systems and services through automation and uplifts. balance product and site reliability with well-defined service-level objectives. make the detailed plan for the tools upgrade and get it implemented in least (or zero) downtime. automated the process of aws resource provisioning using the terraform with best practices.  manage and owning the complete product journey from searching a product to placing an order.  design and configure all the deployment and automation process on the aws, using the set of devops tools. implementation of logging and monitoring tools (such as elk stack, prometheus, grafana, appdynamics, kafka etc.) to easily identify and alert the respective teams to quickly fix the issue (if any). make a cost-effective plan to setup any new microservice or application. manage and configure the aws resources (ec2, eks, lb, r53, sg, code deploy, code pipelines, amis, snapshots, ebs, rds, vpc, subnet, vpc-peering, lambda, cloudwatch etc.) in cost effective manner and do an automated utilization monitoring of these resources to reduce the infrastructure cost. help in reducing the infrastructure cost by implementing smart design. implementations of the aws recommendations to all the resources on the regular interval to reduce the infra cost and enhance the performance.   technology & skills: : jenkins, docker, kubernetes, shell script, github, elk, appdynamics,  aws, kafka, ansible, helm etc  project name: ohp client name: aflac working location: tokyo, japan technology & skills: jenkins, docker, kubernetes, shell script, github, git, veracode, maven, phpunit.  job role: participated in the project development life cycle. design and setup the complete ci-cd pipeline written the deployment automation scripts as per the business requirement lead the devops team.  project name: mobile coe  client: apple + ibm blackbox (jal, singapore airlines, finnair airlines, lufthansa airlines, sas, at&t, amica, coa etc.)  job role:  providing paas and iaas support over ibm cloud (earlier known as ibm bluemix). creating kubernetes cluster for prod, pre-prod and dev environment and maintaining high availability of clusters. troubleshooting mobilefirst platform issues by analyzing logs, checking connectivity to client sor. coordinating with ibm cloud team to resolve platform related issues asap. continuous checking certificates and other dependencies on periodic basis to avoid unplanned outage. setup of mobile first platform on ibm kubernetes service over ibm cloud as well as on client on prim data center. creating and managing docker images over ibm cloud. coordinating with netscaler team to maintain the high availability and disaster recovery for dual geo applications running on ibm kubernetes service (kubernetes). collaboration of code over version controlling using gitlab. written shell script to copy logs, capturing all must gather information during downtime and to check the connections attached to client sor for a specific time. managed middleware services (websphere administration server liberty) on-premises and over cloud. database synchronization in dual geo for mobile first platform applications.  client: etihad airways (aviation industry) job role: deployment of nodejs application over ibm kubernetes services to expose an application through api connect.  api deployment and integration using api connect over ibm cloud. modifying apis on the instruction of architects as per the client requirement in apic. integrated apis with product and pushed it to production. handling update of kubernetes cluster and continuous monitoring of cluster. created a shell script to perform rolling restart after copying the logs based on the alert of heap memory from new relic apm agent. providing platform support for apic, kubernetes, grafana, kibana, and redis over ibm cloud. working on replication of redis in dual geo to avoid the latency issues.  project name: fab (banking) client name: first abu dhabi bank  technology & skills: docker, kubernetes, shell script, api connect, mfp, ibm cloud private, gitlab. job role: participated in multi-phases of software development life cycle (sdlc) of the project including design, implementation, and testing. installation of gitlab, api connect, mobilefirst platform, and db2 in ibm cloud private environment. writing docker file for mobilefirst, api connect, analyzing yaml files for any modification in ingress or other services. troubleshooting and fixing environmental errors, pipeline errors, kubernetes errors, etc. languages  english – absolute fluency |hindi – native language",
    "abhishek   kumar   data engineer      contact      address   sector 87, faridabad   phone   +918376035753   e-mail   abhishekqumar @gmail.com   linkedin   https://www.linkedin.com/in /theabhisrt/      skills      languages: sql,  python   etl tools: talend big  data, abinitio   big data tools: spark,  hive, hdfs, hadoop,  sqoop , azure data  factory,pyspark   databases: oracle 11.2 db,  db2  cloud platform:  google  cloud platform,  microsoft  azure   automation: apache  airflow      • worked in cohesion with industry leaders and trendsetters in  pharmacy and retail industry developing intelligent business solutions  with a global impact across over 9000 stores in analytics and  forecasting.   • designed and implemented data pipelines to  process daily  transactional data using spark, hive and python on google cloud  platform .  • build, deployed and managed data infrastructure that can  adequately handle the needs of rapidly growing data driven  organizations .  • consulted for the world's largest pharmacy chain leveraging digital  tools and technologies such as apache spark, ambari, hdfs , spark - sql, hive, sqoop to optimize their inventory forecasting strategies  and offerings      work history (total experience -4 years  7 mon ths)       2021 -09 -  present    senior data engin eer  dunnhumby india,gurgaon, hr   dunnhumby  is the world’s leading customer science  company, empowering   businesses everywhere to compete and thrive in the  modern data -driven economy. here at   dunnhumby we analyze data and apply insights to  create personalized  customer experiences in   digital, mobile, and retail environments     key responsibilities   • collaborated with various  team  & management  to  understand  the requirement  & design the  complete  system   • creating  a complete solution  by integrating a   variety of programming  languages & tools  together   • responsible for loading  data from gcp  buckets to  data lake  and involved  in design and  development  of data  transformation framework  components  to  support etl process   • efficiently  used spark transformations  and actions to  build simple/quick  and complex etl  application   using pyspark and sparksql   misc: gitlab, unix , google  data studio,  agile, sql  deve loper, toad,  cloudera ,hortonworks   distribution , jira  • implemented the deployment with ci/cd pipelines  using gitlab   • develop and deploy the outcome  using spark and  py spark code in hadoop  cluster running on  gcp   • efficient in creating, debugging, scheduling and  monitoring jobs using airflow   • published the influx db data on grafana and  created grafana dashboards   • submit spark jobs using gsutil and spark submission  get it executed in dataproc cluster                  2019 -02 –  2021 -09   talend big data developer - se  tata consultancy services,noida, up   worked  in tcs  with for of 2 years and 6 month  in big  data analytics and talend big data etl   key responsibilities   • created and deployed numerous talend jobs in  production environment under abinitio to talend  migration drive   • optimized and rebuilt obsolete code and processes  to acquire meaningful data using spark -sql, hive,  sqoop, hdfs, hadoop for analysis and forecasting   • extensively used sql and spark -sql for design and  re-architecture of mapreduce based workflows to  achieve faster performance   • performed hadoop administration tasks such as  monitoring health and cleanup across nodes to  ensure accessibility   • created data -driven workflows for orchestrating  data movement and transf ormed data at scale  using azure data factory   key achievements   • designed and implemented various automated big  data talend etl jobs which cut down on operation,  server and support costs account -wide   • upgraded and pipelined job monitoring and  dependency valida tion using yarn, ambari to  effectively run big data jobs and mitigate support  costs and manhours      certifications and awards        2021 -03   microsoft certified - azure data fundamentals (dp-900)     2021 -03   microsoft certified - azure fundamentals(az -900)     2020 -09   all about hadoop - big data certification     2019 -10   tcs best team  award     2019 -08   tcs on the spot award      education        2014 -07 -  2018 -08   b.tech : cse   sdiet  - faridabad     2013 -01 -  2014 -04   senior secondary   sos herman gmeiner school  - faridabad     2011 -02 -  2012 -04   matriculation   shraddha mandir  school - faridabad     .",
    "adarsh raj   bengaluru, india     adarsh18raj@gmail.com     +91-9206543619      linkedin.com/in/ adarsh18raj    summary   senior data engineer ing professional  with a deep understanding of diverse technologies and methodologies, adept at building  efficient etl pipelines, developing robust applications, and implementing data warehouses using cutting -edge cloud platforms.   experience   senior software engineer - data  engineering   alopa  september 2020 - june 2023, bengaluru   • developed and maintained multiple snowflake data pipelines , using apache airflow  for automation and dbt  for efficient  data  transformation, leading to rapid insights.   • achieved a monumental 6x acceleration  in data operations, and decreased data discrepancies by 50%  using sql validation.   • optimized snowflake architecture for security, performance, and maintenance .  • developed a reusable data model , enhancing customer forecas ting accuracy by  35% .  data engineering intern   everlytics  january 2020 - june 2020, bengaluru   • designed and implemented etl pipelines  using python , spark , sql , and apache airflow , moving data from s3 to snowflake   with dbt transformations .  • enhanced snowflake efficiency ( stages, tables, views, functions ), boosting etl speeds by 40%  and aiding data ingestion.   • automated data processes, enhancing speed by 200%  and slashing data cleaning time by 90% .  • scraped data  from various web sources, deriving insights  via pattern identification  and predictive modeling.   project   data management for a day -care software   alopa • may 2022 - may 2023   • architected data workflows via apache airflow , boosting data availability by 75%  in daycare software.   • spearheaded postgresql  database initiatives for seamless data transactions.   • fortified data precision with strategic modeling  and rigorous testing , curbing errors by 40% .  • orchestrated automated data protocols for 3,000+ daycare centers, guaranteeing impeccable accuracy without interruptions .  database migration - oracle db to snowflake   alopa • august 2021 - may 2022   • migrated oracle db to snowflake  with dbt , advanced sql , and  python scripts  for data processing.   • optimized database performance by 40% , reducing data storage costs by 25% , and increased query response times by 65% .  • streamlined daily report runtime from ~ 6 hours to 10 minutes ; added business logic for data conformity during transformation.   python backend for a healthcare company software   alopa • october 2020 - july 2021   • developed rest api  endpoints for healthcare software, utilizing python , rest framework , aws , and mysql .  • improved response time of the endpoints by 50% , optimized backend performance  to reduce api calls by 33% , and ensured  that api code adhered to best standards , promoting stability  and scalability .  • automated manual processes and implemented automatic testing , reducing manual testing time by 25% , saving 6 -8 hours  of  weekly labor.   education   bachelor of engineering (b.e.) in computer science and engineering  (2016 – 2020 )  kle technological university • hubballi, ka, india   skills   programming  languages: python, sql, javascript   technical skills: software development, data engineering, data modelling, etl/elt pipeline development, cloud engineering  (aws, gcp, azure)   tools: flask, fastapi, mongodb, mysql, postgresql, pandas, numpy, requests, beautiful soup , scrapy , selenium, snowflake,  google bigquery, apache airflow, dbt, aws, gcp, docker, data structures, algorithms, git, source control   soft skills: problem -solving, teamwork, time management, leadership",
]

tokenizer = Tokenizer()
bm25 = BM25Plus(tokenizer.tokenize_batch(resumes))
encoder = Embeddings()
encoder.encode(resumes)
ranker = ReRank()

while True:
    query = input("Enter query: ")
    if query == "/exit":
        break

    re = set()

    bm25_scores = bm25.get_scores(tokenizer.tokenize(query))
    top_n = np.argpartition(bm25_scores, -3)[-3:]
    bm_25_hits = [(bm25_scores[i], resumes[i], i) for i in top_n]
    bm_25_hits.sort(key=lambda x: -x[0])
    print("BM25 hits:")
    for score, resume, i in bm_25_hits:
        re.add(i)
        print(f"{score:.2f} - {i}: {resume[:100]}...")

    bi_encoder_hits = encoder.search(query)
    print("\n\nBi-encoder hits:")
    for el in bi_encoder_hits:
        i = el["corpus_id"]
        re.add(i)
        print(f"{el['score']} - {i}: {resumes[i][:100]}...")

    re_hits = list(re)
    cross_in = [[query, resumes[i]] for i in re_hits]
    cross_scores = ranker.rerank(cross_in)

    print("\n\nCross-encoder hits:")
    hits = zip(re_hits, cross_scores)
    hits = sorted(hits, key=lambda x: -x[1])[:3]
    for i, score in hits:
        if score < 0.5:
            break
        print(f"{score} - {i}: {resumes[i][:100]}...")
